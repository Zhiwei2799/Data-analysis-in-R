---
title: "Insurance charge data analysis"
author: "Zhiwei Lin"
date: "2023-01-24"
output: html_document
---
```{r}
library(tidyverse)
library(skimr)
library(car)
library(MASS)
library(GGally)
library(ggpmisc)
```
### load dataset
```{r}
data<-read.csv("/kaggle/input/insurance/insurance.csv", header =T, na.string=c("","NA"))
```

```{r}
head(data)
str(data)
```
View the first 6 rows of the dataset and the class of each variable in the dataset. 

```{r}
data <- mutate_at(data, vars(sex,smoker,region), as.factor) 
```

The variables 'sex', 'smoker', and 'region' should be converted from character variables to factor variables as they have a limited number of levels.

```{r}
data<-distinct(data) 
```
Remove duplicate rows based on all columns 

### Data Summarization
```{r}
summary(data)
```

```{r}
skim_without_charts(data) # another summary function
```
The summary function shows that there is no missing values in the dataset.

### Data Visualization
```{r}
df_sex<-data %>% 
  group_by(sex) %>% 
  summarise(
    count=n()
  )
df_sex$percentage <- 100*prop.table(df_sex$count)

print(df_sex)

ggplot(df_sex, aes(x="", y=percentage, fill=sex)) +
  geom_bar(width=1,stat="identity") +
  coord_polar(theta="y", start=0) +
  theme_void()+
  labs(title="Pie Chart of Female vs. Male ", fill="sex")+
   geom_text(aes(label = paste(round(percentage,2), "%")),position = position_stack(vjust = 0.5),color = "black")
```
The dataset shows a roughly equal proportion of male and female participants, with 49.51% being female and 50.49% being male, indicating that the data is not biased towards any one gender.

```{r}
df_region<-data %>% 
  group_by(region) %>% 
  summarise(
    count=n()
  )
df_region$percentage <- 100*prop.table(df_region$count)

print(df_region)

ggplot(df_region, aes(x="", y=percentage, fill=region)) +
  geom_bar(width=1,stat="identity") +
  coord_polar(theta="y", start=0) +
  theme_void()+
  labs(title="Pie Chart of Female vs. Male ", fill="region")+
   geom_text(aes(label = paste(round(percentage,2), "%")),position = position_stack(vjust = 0.5),color = "black")
```
the dataset shows roughly equal proportion of four regions.

```{r}
df_age<-data %>% 
  group_by(age) %>% 
  summarise(
    number=n()
  )
print(df_age)
ggplot(df_age, aes(x=age,y=number, fill=age)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = number), position = position_stack(vjust=1.1), angle = 90, size = 3)+
  ggtitle("Age Distribution") +
  xlab("Age") +
  ylab("Count")
```
The dataset includes participants aged between 18 and 64, with a majority of around 25 participants per age group. Notably, the age groups of 18 and 19 stand out with more participants, about 65 individuals, than the other age groups.

```{r, warning=FALSE, message = FALSE}
ggpairs(data)
```
scatterplot matrix with all variables in the dataset

```{r, warning= FALSE, message=FALSE}
ggplot(data,aes(x=age,y=charges,fill=smoker, color=smoker))+geom_point()+ stat_poly_line() + stat_poly_eq()
ggplot(data,aes(x=age,y=charges,fill=sex, color=sex))+geom_point()+ stat_poly_line() + stat_poly_eq()
ggplot(data,aes(x=age,y=charges,group=children, fill=children, color= children ))+geom_point()+ stat_poly_line(se=FALSE) + stat_poly_eq()
ggplot(data,aes(x=bmi,y=charges,fill=region, color=region))+geom_point()+stat_poly_line(se=FALSE) + stat_poly_eq() 
```
the scatter plots of charges vs. age, with each plot filled by a categorical variable such as "smoker","sex", "children" and "region" The group of individuals who is smoker or having 5 children appears to have a high R-squared value. 
```{r}
ggplot(data,aes(x=bmi,y=charges,fill=smoker, color=smoker))+geom_point()+ stat_poly_line() + stat_poly_eq()
ggplot(data,aes(x=bmi,y=charges,fill=sex, color=sex))+geom_point()+ stat_poly_line() + stat_poly_eq()
ggplot(data,aes(x=bmi,y=charges,group=children, fill=children, color= children ))+geom_point()+ stat_poly_line(se=FALSE) + stat_poly_eq()
ggplot(data,aes(x=bmi,y=charges,fill=region, color=region))+geom_point()+stat_poly_line(se=FALSE) + stat_poly_eq() 
```
the scatter plots of charges vs. bmi, with each plot filled by a categorical variable such as "smoker","sex", "children" and "region" The group of individuals who is smoker appears to have a high R-squared value. 

### multiple linear regression
```{r}
fit <- lm(charges ~ age + sex + bmi + children + smoker + region, data=data)
summary(fit)
```
age, bmi children, smokeryes, southeast and southwest are the variables or levels which apper to have significant impact on the insurance charges. However, we must check the assumption of this multiple linear regession before making conclusion. 

```{r}
par(mfrow=c(2,2))
plot(fit)
```
```{r}
residual <- residuals(fit)
shapiro.test(residual) # check for normality assumption.
```

```{r}
vif(fit)
```


Assumption: 
Linearity: The residual vs. fitted plot indicates that the residuals are not randomly scattered around the horizontal line of zero. This indicates that the linear model may not be the best fit for the data. Not satisify
Normality: the Normal Q-Q plot and shapiro test indicate the residuals are not normally distributed. Not satisify
No multicollinearity: vif test shows there are no multicollinearity in the model, which is good
Homogeneity: scale-location plot shows red line is not horintonal with ponts spread equally across the plot. This indicates heteroscedasticity exists.


### box-cox tranforamtion
```{r}
bc <- boxcox(charges ~ age + sex + bmi + children + smoker + region , data=data)
(lambda <- bc$x[which.max(bc$y)])
```

```{r}
new_model <- lm(((charges^lambda-1)/lambda) ~ age + sex + bmi + children + smoker + region, data=data)
summary(new_model)
```
```{r}
par(mfrow=c(2,2))
plot(new_model)
```
Transforming variables does not to satisify the assumptions of linear model. Hence, we should perform decision tree-based method like random forest. 


### Random forest regression 
```{r}
library(randomForest)
library(caret)
```

```{r}
set.seed(123)
training.samples <- data$charges %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- data[training.samples, ]
test.data <- data[-training.samples, ]
```
split dataset into train and test data, with 80% being train data and 20% being test data. 

```{r}
set.seed(123)
model <- train(
  charges ~., data = train.data, method = "rf",
  trControl = trainControl("cv", number = 10),
  importance = TRUE
  )
```
10-fold cross-validation would be used

```{r}
model$bestTune #best set of tuning parameter.
```
```{r}
varImpPlot(model$finalModel, type = 1)
varImpPlot(model$finalModel, type = 2)
varImp(model)
```
Smokeryes is the most significant variable in determining insurance charges, followed by BMI and age. Other factors have minor or no impact on the charges

```{r}
predictions <- model %>% predict(test.data)
head(predictions)
```
```{r}
RMSE(predictions, test.data$charges)
```
The root mean squared error between the predicted charges and the actual charges in the test data is 4806.051.


